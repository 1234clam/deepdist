<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.cssx" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>DeepDist by Dirk Neumann</title>
  </head>

  <body>
    <style>
    .preline
    </style>
    <div id="container">
      <div class="inner">

        <header>
          <h1>DeepDist</h1>
          <h2>Lightning-Fast Deep Learning on <a href="https://spark.apache.org/" target="_blank">Spark</a></h2>
        </header>
        <!--
        <section id="downloads" class="clearfix">
          <a href="https://github.com/dirkneumann/deepdist/zipball/master" id="download-zip" class="button"><span>Download .zip</span></a>
          <a href="https://github.com/dirkneumann/deepdist/tarball/master" id="download-tar-gz" class="button"><span>Download .tar.gz</span></a>
          <a href="https://github.com/dirkneumann/deepdist" id="view-on-github" class="button"><span>View on GitHub</span></a>
          <br>&nbsp;<br>&nbsp;</br>
          git clone https://github.com/dirkneumann/deepdist.git
        </section>

        <hr>
        -->
        <section id="main_content">
          <h3>Introduction</h3>
          <p>Training deep belief networks requires extensive data and computation.  DeepDist accelerates the training by distributing stochastic gradient descent for data stored on HDFS / Spark via a simple Python interface.</p>
          <h3>
<a name="welcome-to-deepdist" class="anchor" href="#welcome-to-deepdist"><span class="octicon octicon-link"></span></a>Quick start</h3>
          <p> Let's train a <a href="https://code.google.com/p/word2vec/" target="_blank">word2vec</a> model on all of <a href="http://dumps.wikimedia.org/enwiki/" target="_target">wikipedia</a> in 15 lines of code:</p>
          <script src="https://gist.github.com/dirkneumann/12e0d7cea39ecc82c10d.js"></script>
          <p>The model can be tested with 'woman' - 'man' + 'king' and computes: 'queen'.</p>
          
          <h3>How does it work?</h3>
          <p>DeepDist implements a <a href="http://research.google.com/archive/large_deep_networks_nips2012.html" target="_blank">Sandblaster</a>-like stochastic gradient descent.  It start a master model server (on port 5000).  On each data node, DeepDist fetches the model from the server, and then calls <strong>gradient()</strong>.  After computing the gradient for each RDD partition, gradient updates are send the the server.  On the server, the master model is then updated by <strong>descent()</strong>.</p>
          <img src="images/deepdistdesign.png"/>
          
          <p><strong>Figure 1</strong>. The model is store on the master node and served on port 5000.  The compute nodes fetch the model before processing each partition, and send the gradient updates back the server.  The server can perform stoachastic gradient descent (or other optmization procedures) with the node updates.
          
          <h3>API</h3>
          <pre><code>class DeepDist:

    def __init__(self, model):
        # initialized the model server
    
    def train(self, corpus, gradient, descent):
        # accepts a data RDD, and gradient and descent callback methods

def gradient(model, data):
    # model is a copy of the master model
    # data is an iterator for the current partition of the data RDD
    # returns the gradient update

def descent(model, update):
    # model is a reference to the server model
    # update is a copy of a worker's update
          </code></pre>
          
          <h3>Training Speed</h3>
          
          <p>Training speed can be greatly enhanced by adaptively adjusting the learning rate by <a href="http://www.cs.berkeley.edu/~jduchi/projects/DuchiHaSi10.pdf" target="_blank">AdaGrad</a>.  The complete Word2Vec model with 900 dimensions can be training on a 19GB wikipedia corpus with DeepDist.</p>
          <img src="images/training.png"/>
          <p><strong>Figure 2</strong>.  Performance on a test set (analogy questions) after training up to 100,000 parallel epochs of a Word2Vec model with stochastic AdaGrad gradient descent.</p>
          
          <h3>Addition Information</h3>
          <table><tr><td>DeepDist:</td><td><a href="https://github.com/dirkneumann/deepdist/" target="_blank">https://github.com/dirkneumann/deepdist</a><br>&nbsp;</td></tr>
          <tr><td>Wikipedia Data: &nbsp; </td><td> <a href="https://dumps.wikimedia.org/enwiki/" target="_target">https://dumps.wikimedia.org/enwiki</a></td></tr>
          <tr><td>Text from dumps: &nbsp; </td><td> <a href="http://cs.fit.edu/~mmahoney/compression/textdata.html" target="_target">http://cs.fit.edu/~mmahoney/compression/textdata.html</a></td></tr>
          
          </table>
        </section>

        <footer>
          DeepDist is maintained by <a href="https://github.com/dirkneumann">DirkNeumann</a><br>
        </footer>

        
      </div>
    </div>
  </body>
</html>