<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.cssx" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>DeepDist | Lightning-Fast Deep Learning on Spark</title>
  </head>

  <body>
    <style>
    .preline
    </style>
    <div id="container">
      <div class="inner">

        <header>
          <h1>DeepDist</h1>
          <h2>Lightning-Fast Deep Learning on <a href="https://spark.apache.org/" target="_blank">Spark</a></h2>
          <h2>Fast asynchronous stochastic gradient updates</h2>
          By <a href="https://www.linkedin.com/in/dirkneumann" target="_blank">Dirk Neumann</a>
        </header>
        
        <section id="downloads" class="clearfix">
          <a href="https://github.com/dirkneumann/deepdist/zipball/master" id="download-zip" class="button"><span>Download .zip</span></a>
          <a href="https://github.com/dirkneumann/deepdist/tarball/master" id="download-tar-gz" class="button"><span>Download .tar.gz</span></a>
          <a href="https://github.com/dirkneumann/deepdist" target="_blank" id="view-on-github" class="button"><span>View on GitHub</span></a>
        </section>

        <section id="main_content">
          <h3>Introduction</h3>
          <p>Training deep belief networks requires extensive data and computation.  DeepDist accelerates model training by distributing stochastic gradient descent for data stored on HDFS / Spark.</p>
        </section>

        <section>
          <h3>How does it work?</h3>
          <p>DeepDist implements a <a href="http://research.google.com/archive/large_deep_networks_nips2012.html" target="_blank">Sandblaster</a>-like stochastic gradient descent.  It starts a master model server that integrates updates in real time.  On each data node, DeepDist fetches the model from the master, and then calls <strong>gradient()</strong>.  After computing gradients for the RDD partitions, gradient updates are send the the server.  On the server, the master model is updated by <strong>descent()</strong>.  Models can converge faster since gradient updates are constatently synchronized between the nodes.</p>
          <img src="images/deepdistdesign.png"/>
        
          <p><strong>Figure 1</strong>. The model is store on the master node and served on port 5000.  The compute nodes fetch the model before processing each partition, and send the gradient updates back the server.  The server can perform stoachastic gradient descent (or other optmization procedures) with the node updates.
        
          <h3>How do MLlib and Mahout work?</h3>
          
          <p>In contrast, Mahout and MLlib compute all gradients over the (subsampled) data before updating the model.  Gradient updates can be less frequent and deep belief models might converge slower.
          
          <img src="images/mllib.png">
          <p><strong>Figure 2</strong>. Classical machine learning libraries broadcast the model parameters to the nodes, and then compute the gradients in parallel.  At the end of a computational step, the gradients are collected and transfered to the master.</p>
          
          <h3>
<a name="welcome-to-deepdist" class="anchor" href="#welcome-to-deepdist"><span class="octicon octicon-link"></span></a>Quick start</h3>
          <p> Let's train a <a href="https://code.google.com/p/word2vec/" target="_blank">word2vec</a> model on all of <a href="http://dumps.wikimedia.org/enwiki/" target="_target">wikipedia</a> in 15 lines of Python code:</p>
          <script src="https://gist.github.com/dirkneumann/12e0d7cea39ecc82c10d.js"></script>
          <p>The model can be tested with 'woman' - 'man' + 'king' and computes: 'queen'.</p>
          
          <h3>Python module</h3>

          DeepDist provides a simple Python interface.  The <strong>with</strong> statement starts the model server.  Distributed gradient updates are computed on partitions of a resilient distributed dataset (RDD) data.  The gradient updates are incorporated into the master model via custom descent method.
          
          <script src="https://gist.github.com/dirkneumann/7591ec58b14a754a30b7.js"></script>
          <br>
          
          <h3>Scala package</h3>
          
          Are you interested in a Scala version of this package?<br><br>
          
          <h3>Training Speed</h3>
          
          <p>Training speed can be greatly enhanced by adaptively adjusting the learning rate by <a href="http://www.cs.berkeley.edu/~jduchi/projects/DuchiHaSi10.pdf" target="_blank">AdaGrad</a>.  The complete Word2Vec model with 900 dimensions can be trained on the 19GB wikipedia corpus (using the words from the validation questions).</p>
          <img src="images/training.png"/>
          <p><strong>Figure 2</strong>.  Performance on a test set (analogy questions) after training of a Word2Vec model with stochastic AdaGrad gradient descent.</p>
                    
          <h3>References</h3>
          <p>J Dean, GS Corrado, R Monga, K Chen, M Devin, QV Le, MZ Mao, Mâ€™A Ranzato, A Senior, P Tucker, K Yang, and AY Ng. <a href="http://research.google.com/archive/large_deep_networks_nips2012.html" target="_blank">Large Scale Distributed Deep Networks</a>. NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada, 2012.</p>
          <p>T Mikolov, I Sutskever, K Chen, G Corrado, and J Dean. <a href="http://arxiv.org/pdf/1310.4546.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality.</a>  In Proceedings of NIPS, 2013.</p>
          <p>T Mikolov, K Chen, G Corrado, and J Dean. <a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank">Efficient Estimation of Word Representations in Vector Space</a>. In Proceedings of Workshop at ICLR, 2013.</p>

          <hr>

          <h3>Addition Information</h3>
          <br>
          <table><tr><td>DeepDist:</td><td><a href="https://github.com/dirkneumann/deepdist/" target="_blank">https://github.com/dirkneumann/deepdist</a><br>&nbsp;</td></tr>
          <tr><td>Wikipedia data: &nbsp; </td><td> <a href="https://dumps.wikimedia.org/enwiki/" target="_target">https://dumps.wikimedia.org/enwiki</a></td></tr>
          <tr><td>Extracting text: &nbsp; </td><td> <a href="http://cs.fit.edu/~mmahoney/compression/textdata.html" target="_target">http://cs.fit.edu/~mmahoney/compression/textdata.html</a></td></tr>
          
          </table>


        </section>

        <footer>
          DeepDist is maintained by <a href="https://plus.google.com/112739410607400285081?rel=author" target="_blank">DirkNeumann</a><br>
        </footer>

        
      </div>
    </div>
  </body>
</html>
